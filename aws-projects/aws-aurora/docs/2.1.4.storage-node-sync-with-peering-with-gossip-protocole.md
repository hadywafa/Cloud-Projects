# ðŸ— **Amazon Aurora Storage Nodes â€“ Behind the Scenes**

Amazon Aurora's storage architecture is one of the **biggest innovations** in cloud-based relational databases. Unlike traditional RDBMS architectures that store all data in a **single monolithic storage volume**, Aurora uses a **distributed, fault-tolerant, and auto-scaling storage system**.

This document covers **everything you need to know** about Aurora's storage internals, including:
- **How Aurora distributes data** across storage nodes
- **How it ensures durability, consistency, and fault tolerance**
- **How storage nodes communicate and sync data**
- **How Aurora scales beyond 100TB without downtime**
- **How Aurora handles read and write operations efficiently**

Let's dive in! ðŸš€ðŸ”¥

---

## ðŸ“‚ **1. Aurora Storage Layer: High-Level Overview**
### ðŸ” **How is Aurora Storage Different from Traditional RDBMS?**
Unlike traditional databases that store data in a **single disk or volume (e.g., EBS in RDS)**, Aurora breaks the database into **smaller storage segments (Protection Groups)** and spreads them across **multiple storage nodes in different Availability Zones (AZs)**.

### âš™ï¸ **Core Features of Auroraâ€™s Storage Layer**
- **ðŸ“¦ Auto-Scalable Storage:** Aurora automatically expands storage from **10GB to 128TB** without downtime.
- **ðŸŒ Distributed Storage:** The database is **split into 10GB chunks** (Protection Groups) and stored across multiple storage nodes.
- **ðŸ† Fault-Tolerant:** Each 10GB chunk is **replicated six times** across **three AZs**.
- **ðŸŒŽ Globally Distributed:** With Aurora Global Database, data can be **replicated across AWS regions**.

### ðŸ¢ **How Aurora Storage Nodes Are Structured**
Auroraâ€™s **storage nodes** consist of:
- **âœ… Storage Nodes (EC2 Instances):** Compute units responsible for managing storage blocks.
- **ðŸ›¡ Protection Groups:** 10GB data chunks distributed across multiple nodes.
- **ðŸ’° Redo Log Storage:** Instead of modifying data pages immediately, Aurora **writes redo logs first**, which are then applied to storage nodes.
- **ðŸ“¡ Peer-to-Peer Gossip Protocol:** Storage nodes synchronize updates without a central coordinator.
- **ðŸ› Quorum-Based Writes:** A write is confirmed only if **4 out of 6 storage nodes** acknowledge the change.

---

## ðŸ”„ **2. How Aurora Distributes Data Across Storage Nodes**
### ðŸ” **What is Data Distribution in Aurora?**
Aurora **does not store the entire database in a single location**. Instead:
- The database is **split into 10GB chunks (Protection Groups)**.
- These chunks are **spread across multiple storage nodes in different AZs**.
- **Each storage node stores only part of the database (not the full database).**

### âš™ï¸ **Corrected Example: How Aurora Stores a 50GB Database**
| **Shard (Protection Group)** | **Storage Node in AZ1** | **Storage Node in AZ2** | **Storage Node in AZ3** |
|-----------------|----------------|----------------|----------------|
| **Shard 1 (10GB)** | Node A | Node C | Node E |
| **Shard 2 (10GB)** | Node B | Node D | Node F |
| **Shard 3 (10GB)** | Node G | Node H | Node I |
| **Shard 4 (10GB)** | Node J | Node K | Node L |
| **Shard 5 (10GB)** | Node M | Node N | Node O |

### ðŸ’¡ **Key Takeaways:**
- **Each storage node stores only a subset of the database.**
- **Chunks are evenly distributed across three AZs to ensure durability.**
- **If a node fails, data is reconstructed from the remaining nodes.**

---

## ðŸ”„ **3. How Aurora Handles Reads and Writes in a Distributed System**
### **âœï¸ Write Operation Sequence Diagram**
```mermaid
sequenceDiagram
    participant Client as Client
    participant QueryEngine as Aurora Query Engine (Primary DB)
    participant StoragePool1 as Storage Pool (AZ1)
    participant StorageNodeA as Storage Node A (Shard 1)
    participant StorageNodeB as Storage Node B (Shard 2)
    participant StoragePool2 as Storage Pool (AZ2)
    participant StorageNodeC as Storage Node C (Shard 1)
    participant StorageNodeD as Storage Node D (Shard 2)
    participant StoragePool3 as Storage Pool (AZ3)
    participant StorageNodeE as Storage Node E (Shard 1)
    participant StorageNodeF as Storage Node F (Shard 2)

    Client->>QueryEngine: Write Request (INSERT/UPDATE)
    QueryEngine->>StoragePool1: Identify Shard & Send Redo Log
    StoragePool1->>StorageNodeA: Write to Shard 1
    StoragePool1->>StorageNodeB: Write to Shard 2

    QueryEngine->>StoragePool2: Identify Shard & Send Redo Log
    StoragePool2->>StorageNodeC: Write to Shard 1
    StoragePool2->>StorageNodeD: Write to Shard 2

    QueryEngine->>StoragePool3: Identify Shard & Send Redo Log
    StoragePool3->>StorageNodeE: Write to Shard 1
    StoragePool3->>StorageNodeF: Write to Shard 2

    StorageNodeA->>QueryEngine: ACK (Confirmed)
    StorageNodeB->>QueryEngine: ACK (Confirmed)
    StorageNodeC->>QueryEngine: ACK (Confirmed)
    StorageNodeD->>QueryEngine: ACK (Confirmed)
    StorageNodeE->>QueryEngine: ACK (Confirmed)
    StorageNodeF->>QueryEngine: ACK (Confirmed)

    QueryEngine->>Client: Write Committed (Quorum Achieved)
```

---

### **ðŸ“– Read Operation Sequence Diagram**
```mermaid
sequenceDiagram
    participant Client as Client
    participant QueryEngine as Aurora Query Engine (Primary DB)
    participant StoragePool1 as Storage Pool (AZ1)
    participant StorageNodeA as Storage Node A (Shard 1)
    participant StorageNodeB as Storage Node B (Shard 2)

    Client->>QueryEngine: Read Request (SELECT * FROM Orders WHERE order_id = 123)
    QueryEngine->>StoragePool1: Identify Data Partition
    StoragePool1->>StorageNodeA: Fetch Data from Shard 1
    StoragePool1->>StorageNodeB: Fetch Data from Shard 2
    StorageNodeA->>QueryEngine: Return Data
    StorageNodeB->>QueryEngine: Return Data
    QueryEngine->>Client: Return Query Result
```

---

## ðŸŽ¯ **Conclusion: Why Auroraâ€™s Storage Architecture is Revolutionary**
- **Aurora automatically distributes data into 10GB chunks.**
- **Storage nodes handle only a portion of the database.**
- **Queries are executed in parallel across storage nodes for performance.**
- **Writes use Quorum-based commits to ensure durability.**

ðŸ’¡ **Final Thought:** Aurora's storage model **eliminates replication lag, enables instant failover, and scales effortlessly** beyond traditional RDBMS architectures.

ðŸš€ **Would you like a deep dive into how Aurora handles crash recovery and self-healing? Let me know!**

